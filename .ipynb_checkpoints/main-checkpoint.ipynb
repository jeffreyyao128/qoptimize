{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8-yYZglGODU",
    "outputId": "6a9ebd71-7435-4a97-db10-5f2fcf195af3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "import jax\n",
    "from jax import jit,vmap,grad\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import vjp\n",
    "from qutip import *\n",
    "from jax.experimental import ode,optimizers\n",
    "\n",
    "from jax.config import config   \n",
    "config.update(\"jax_enable_x64\", True) # Force Jax use float64 as default Float dtype\n",
    "\n",
    "from jax.tree_util import tree_unflatten,tree_flatten\n",
    "from jax.test_util import check_grads # import jax implemented method to check the grads\n",
    "\n",
    "N = 4 # set number of qubits as a global parameter\n",
    "N1 = 10 #fourier series size\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1cKxLXnVGODZ"
   },
   "outputs": [],
   "source": [
    "def normalize(psi):\n",
    "    '''\n",
    "    Normalize a given wavefunction\n",
    "    '''\n",
    "    return psi/jnp.linalg.norm(psi,axis=0,keepdims=True)\n",
    "\n",
    "def generate_batch(U,key,batch_size=10,batch_num=3,*args):\n",
    "    '''\n",
    "    To generate a bach of data point given a unitary gate\n",
    "    '''\n",
    "    key_re , key_im = random.split(key)\n",
    "    psi_batch = random.normal(key_re,shape=(2**N,batch_size,batch_num)) + 1j*random.normal(key_im,shape=(2**N,batch_size,batch_num))\n",
    "    psi0_batch = jnp.einsum('ijk,li->ljk',psi_batch,U)\n",
    "    #      initial psi          training data psi0\n",
    "    return normalize(psi_batch), normalize(psi0_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPSCTqnOGODc"
   },
   "source": [
    "To perceed neural network, an ODE solver is needed. In our case is `scipy.integrate.solve_ivp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "id": "gyjrFww7GODc"
   },
   "outputs": [],
   "source": [
    "def forward(t1,psi0,flat_p,func_method='matrix'):\n",
    "    '''\n",
    "    solver for Schrodinger equation with initial condition of psi0 and Hamiltonian in the interval of [0,t1]\n",
    "    use mathod of scipy\n",
    "    '''\n",
    "    if func_method == 'function':\n",
    "        Hz = lambda t,x: -1.0j*H(t,x,flat_p,t1)\n",
    "    elif func_method == 'matrix':\n",
    "        def Hz(t,x,*args): \n",
    "            t1,flat_p, = args\n",
    "            return -1.0j*Hmat(t,x,flat_p,t1)\n",
    "    else:\n",
    "        raise NameError(\"wrong solver\")\n",
    "    \n",
    "    sol = solve_ivp(Hz,[0,t1],psi0,t_eval=[0,t1],args=[t1,flat_p],method='RK45') # There are something wrong with the precision control in solve_ivp\n",
    "    return sol.y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "J649wAPpGODf"
   },
   "outputs": [],
   "source": [
    "def initial(key1 = None):\n",
    "    '''\n",
    "    Return randomized parameters\n",
    "    '''\n",
    "    if (key1).all() == None:\n",
    "        return np.random.normal(size=(N1*(N+1)+1,))\n",
    "    else:\n",
    "        return random.normal(key1,shape=(N1*(N+1)+1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "id": "QVz6THRdGODj"
   },
   "outputs": [],
   "source": [
    "def n(i,x):\n",
    "    '''\n",
    "    number operator for n_i (x)\n",
    "    '''\n",
    "    return (x&2**i)/2**i\n",
    "\n",
    "def unpackp(x):\n",
    "    return jnp.array(x[:N1]) , jnp.array([x[(i+1)*N1:(i+2)*N1] for i in range(N)]), jnp.array(x[-1], dtype=jnp.float32)\n",
    "\n",
    "def flatten(x):\n",
    "    return 0 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPT96vU6ZZ4r"
   },
   "source": [
    "Our Hamiltonian is $$\\begin{aligned}\n",
    "\\frac{H}{\\hbar}=& \\frac{\\Omega(t)}{2} \\sum_{i=1}^{N} \\sigma_{x}^{(i)}-\\sum_{i=1}^{N} \\Delta_{i}(t) n_{i} \\\\\n",
    "&+\\sum_{i<j} \\frac{V}{|i-j|^{6}} n_{i} n_{j}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "zBCGwRkgGODl"
   },
   "outputs": [],
   "source": [
    "def H(t,psi,flat_p,t1):\n",
    "    '''\n",
    "    Using xor to implement off diagonal element\n",
    "    '''\n",
    "    D, = jnp.shape(psi)\n",
    "    res = jnp.zeros(D,dtype=jnp.complex64)\n",
    "\n",
    "    w = 2*jnp.pi/t1\n",
    "\n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    omega = jnp.sum(jnp.array([u_omega[i]*jnp.sin(w*(i+1)*t) for i in range(N1)]))\n",
    "    delta = [jnp.sum(jnp.array([u_d[j,i]*jnp.sin(w*(i+1)*t) for i in range(N1)])) for j in range(N)]\n",
    "    \n",
    "    for x in range(D):\n",
    "        Ci = psi[x]\n",
    "        diag = - jnp.sum(jnp.array([ delta[i]*n(i,x) for i in range(N)]))\\\n",
    "        + jnp.sum(jnp.array([[V*n(i,x)*n(j,x)/jnp.abs(i-j)**6 if i<j else 0 for i in range(N)] for j in range(N)])) #diagonal part of hamiltonian\n",
    "        res = jax.ops.index_add(res, (x,), diag*Ci)\n",
    "        cast = jnp.array([x^2**i for i in range(N)]) # ^ is for xor, to calculate the flip operation\n",
    "        res = jax.ops.index_add(res, cast, omega*Ci/2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed we use dense matrix to represent Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     5,
     23
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "n0 = jnp.array([jnp.diag(jnp.array([n(i,x) for x in range(2**N)])) for i in range(N)]) #number operator matrix form n0[i] = n_i\n",
    "\n",
    "print(n0.shape)\n",
    "#print(jnp.matmul(n0[1],n0[2]))\n",
    "\n",
    "def H_independent():\n",
    "    '''\n",
    "    Time independent part of Hamiltonian\n",
    "    '''\n",
    "    res = jnp.zeros((2**N,2**N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if j<= i: continue\n",
    "            params = 1/np.abs(i-j)**6\n",
    "            res += params*(jnp.dot(n0[i],n0[j]))\n",
    "    return res\n",
    "H1 = H_independent()\n",
    "# print(jnp.trace(H1))\n",
    "\n",
    "f = lambda x,y,i : 1 if y==x^2**i else 0\n",
    "H2 = sum([jnp.array([[f(x,y,i) for x in range(2**N)] for y in range(2**N)]) for i in range(N)]) #checked\n",
    "\n",
    "@jit\n",
    "def Hmat(t,psi,flat_p,t1):\n",
    "    '''\n",
    "    Using dense matrix to represent Hamiltonian\n",
    "    '''\n",
    "    D, = jnp.shape(psi)\n",
    "    res = jnp.zeros(D,dtype=jnp.complex64)\n",
    "    \n",
    "    w = 2*jnp.pi/t1\n",
    "    \n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    \n",
    "    ft =jnp.array([jnp.sin(w*(i+1)*t) for i in range(N1)])\n",
    "\n",
    "    omega = jnp.dot(u_omega,ft)\n",
    "    delta = u_d@ft\n",
    "    \n",
    "    return (V*H1 + 0.5* omega* H2 - jnp.einsum('i,ijk->jk',delta,n0))@psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "id": "Voa8tv0HGODo"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(psi,flat_p,t1,A,psi0):\n",
    "    '''\n",
    "    define the loss function, which is a pure function\n",
    "    '''\n",
    "    omega = 2*jnp.pi/t1\n",
    "    l1 = 1 - jnp.abs(jnp.dot(jnp.conjugate(psi),psi0))**2 # Overlap\n",
    "    \n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    freqeuncy = jnp.array([i*omega**2 for i in range(N1)])\n",
    "    l2 = 0.5*jnp.dot(jnp.conjugate(freqeuncy),u_omega**2)\n",
    "#     l2 = 0.5*jnp.sum(jnp.array([ (i*omega*flat_p[i])**2 for i in range(N1)]))\n",
    "    l3 = 0.5*jnp.sum(jnp.dot(u_d**2,jnp.conjugate(freqeuncy)))\n",
    "#     l3 = 0.5*jnp.sum(jnp.array([ (i*omega*flat_p[i+N1])**2 for i in range(N1)]))\n",
    "    l4 = 0.5*V**2\n",
    "    \n",
    "    l5 = t1**2 # Punish total time\n",
    "    \n",
    "    return A[0]*l1+A[1]*l2+A[2]*l3+A[3]*l4+A[4]*l5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "ak3mbk0YGODq"
   },
   "outputs": [],
   "source": [
    "def grad_all(psi1,psi0,flat_p,t1,A,method='matrix'):\n",
    "    '''\n",
    "    gradient with adjoint method\n",
    "    return value and grad\n",
    "    '''\n",
    "    if method=='matrix':\n",
    "        H_evol = Hmat\n",
    "    elif method == 'function':\n",
    "        H_evol = H\n",
    "    else:\n",
    "        raise NameError('Wrong dynamics')\n",
    "        \n",
    "    D, = jnp.shape(psi1)\n",
    "    \n",
    "    loss_val, a0 = jax.value_and_grad(loss, argnums=0)(psi1,flat_p,t1,A,psi0)\n",
    "\n",
    "    pLt1 = jnp.array([jnp.dot(a0,H_evol(t1,psi1,flat_p,t1))])\n",
    "    \n",
    "    aug_state = jnp.concatenate([psi1, a0, -pLt1, jnp.zeros(((N+1)*N1+1,))])\n",
    "    print(aug_state.shape)\n",
    "    def unpack(x):\n",
    "              # z , vjp_z   , vjp_t , vjp_args\n",
    "        return x[:D], x[D:2*D], x[2*D], x[2*D+1:]\n",
    "    \n",
    "    def wrap_vjp(augment_state,t,parameters):\n",
    "        Hz = lambda z : H_evol(t,z,parameters,t1)\n",
    "        Ht = lambda t0 : H_evol(t0,augment_state[0],parameters,t1)\n",
    "        Hp = lambda p : H_evol(t,augment_state[0],p,t1)\n",
    "        _, vjp_funz = vjp(Hz, augment_state[0])\n",
    "        _, vjp_funt = vjp(Ht, t)\n",
    "        _, vjp_funp = vjp(Hp, flat_p)\n",
    "        \n",
    "        # vjp_funz(augment_state[1]) is a tuple\n",
    "#         print(vjp_funz(augment_state[1])[0].shape,vjp_funp(augment_state[1])[0].shape,vjp_funt(augment_state[1])[0].shape)\n",
    "        return [- vjp_funz(augment_state[1])[0],- vjp_funp(augment_state[1])[0],- vjp_funt(augment_state[1])[0]]\n",
    "    \n",
    "    def aug_dynamics(t,augment_state,*args):\n",
    "        parameters, = args\n",
    "        unpacked_aug_state = unpack(augment_state)\n",
    "\n",
    "        vjps = wrap_vjp(unpacked_aug_state,t,parameters)\n",
    "        aug_grad = jnp.concatenate([H_evol(t,unpacked_aug_state[0],parameters,t1), vjps[0], vjps[1], jnp.array([vjps[2]])])\n",
    "\n",
    "        return aug_grad\n",
    "\n",
    "    aug_state0 = solve_ivp(aug_dynamics,[t1,0],aug_state,t_eval=[t1,0],args=(flat_p,)) # Couldn't use jit for this, since it has no fixed memory.\n",
    "    return loss_val, aug_state0.y[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJSUd71THAQq"
   },
   "source": [
    "Our Hamiltonian is \n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{H}{\\hbar}=& \\frac{\\Omega(t)}{2} \\sum_{i=1}^{N} \\sigma_{x}^{(i)}-\\sum_{i=1}^{N} \\Delta_{i}(t) n_{i} \\\\\n",
    "&+\\sum_{i<j} \\frac{V}{|i-j|^{6}} n_{i} n_{j}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "tSyOBEQjtKyS"
   },
   "outputs": [],
   "source": [
    "def check_with_qutip(psi0,flat_p,t1):\n",
    "    q_psi = Qobj(psi0,dims=[[2,2,2,2], [1 ,1,1,1]])\n",
    "\n",
    "    t = np.linspace(0,t1,100)\n",
    "    w = 2*jnp.pi/t1\n",
    "\n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    \n",
    "    ft = lambda t: jnp.array([jnp.sin(w*(i+1)*t) for i in range(N1)])\n",
    "    omega = lambda t,args: jnp.dot(u_omega,ft(t))\n",
    "    delta = lambda t,args: -u_d@ft(t)\n",
    "    delta_func = [lambda t,args: delta(t,args)[i] for i in range(N1)]\n",
    "#     omega = lambda t,args : jnp.sum(jnp.array([u_omega[i]*jnp.sin(w*(i+1)*t) for i in range(N1)]))\n",
    "#     delta_func = [lambda t,args : - jnp.sum(jnp.array([u_d[j,i]*jnp.sin(w*(i+1)*t) for i in range(N1)])) for j in range(N)]\n",
    "    h = []\n",
    "\n",
    "    si = qeye(2)\n",
    "    sx = sigmax()\n",
    "    sy = sigmay()\n",
    "    sz = sigmaz()\n",
    "    ni = num(2)\n",
    "    # time independent part\n",
    "    h_list = []\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if j<= i: continue\n",
    "            param = V/np.abs(i-j)**6\n",
    "            op_list=[si for _ in range(N)]\n",
    "            op_list[i] = ni\n",
    "            op_list[j] = ni\n",
    "            h_list.append(param*tensor(op_list))\n",
    "    H0 = sum(h_list)\n",
    "#     print(H0.tr()/V)\n",
    "    h.append(H0)\n",
    "    \n",
    "    #time dependent part\n",
    "    sx_list = []\n",
    "\n",
    "    for n in range(N):\n",
    "        op_list = []\n",
    "        for m in range(N):\n",
    "            op_list.append(si)\n",
    "\n",
    "        op_list[n] = sx\n",
    "        sx_list.append(tensor(op_list))\n",
    "\n",
    "    # construct the hamiltonian\n",
    "    H1 = sum([0.5 * sx_list[n] for n in range(N)])\n",
    "    h.append([H1,omega])\n",
    "\n",
    "    for i in range(N):\n",
    "        n_list = [si for _ in range(N)]\n",
    "        n_list[i] = ni\n",
    "        H2= tensor(n_list)\n",
    "        h.append([H2,delta_func[i]])\n",
    "    output = mesolve(h, q_psi, t, progress_bar = True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to use `jax.experimental.ode.odeint` to autograd our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def jax_fwd(psi_init,t1,flat_p,psi0,Loss_vec):\n",
    "    '''\n",
    "    Implement forward odeint with jax.experimental.ode.odeint\n",
    "    \n",
    "    psi_init: a batch of \n",
    "    \n",
    "    return loss\n",
    "    '''\n",
    "    D,batch_size = jnp.shape(psi_init)\n",
    "    \n",
    "    def func(y,t,*args):\n",
    "        t1,flat_p, = args\n",
    "        return -1.0j*Hmat(t,y,flat_p,t1)\n",
    "    t_set = jnp.linspace(0.,t1,5)\n",
    "    \n",
    "    res_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        res = odeint(func,psi_init[:,i],t_set,t1,flat_p,rtol=1.4e-10, atol=1.4e-10)\n",
    "        psi_final = res[-1,:]\n",
    "        res_list.append(psi_final)\n",
    "\n",
    "    return jnp.sum(jnp.array([loss(res_list[i],flat_p,t1,Loss_vec,psi0[:,i]) for i in range(batch_size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def q_optimize(data_batch,Loss_vector,flat_params,t1,learning_rate=1.,mass=1.,num_steps=5):\n",
    "    '''\n",
    "    Train the neural ode over a given databatch\n",
    "    '''\n",
    "    psi_batch,psi0_batch = data_batch # we try with single wavefunction for instance\n",
    "    \n",
    "    D,batch_size,batch_num = jnp.shape(psi_batch)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.momentum(learning_rate,mass) # Use momentum as optimizer\n",
    "    loss_list = []\n",
    "    for i in range(batch_num):\n",
    "        psi,psi0 = psi_batch[:,:,i],psi0_batch[:,:,i]\n",
    "        aug_params = jnp.concatenate([jnp.array([t1]),flat_params])\n",
    "        L = aug_params.size\n",
    "\n",
    "        opt_state = opt_init(aug_params)\n",
    "\n",
    "        def unpack(x):\n",
    "                  # t , args\n",
    "            return x[0], x[1:]\n",
    "\n",
    "        def step_fun(step, opt_state,psi,psi0):\n",
    "    #         value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state))\n",
    "            aug_params = get_params(opt_state)\n",
    "            \n",
    "            t1,flat_params = unpack(aug_params)\n",
    "#             res = forward(t1,psi,flat_params,method='matrix') # Forward with scipy RK45\n",
    "#             value, grads = grad_all(res,psi0,flat_params,t1,Loss_vector) # grad with self \n",
    "#             opt_state = opt_update(step, jnp.real(grads[-L:]), opt_state)\n",
    "            value,grads = jax.value_and_grad(jax_fwd,(1,2))(psi,t1,flat_params,psi0,Loss_vector)\n",
    "            g_t, g_p = grads\n",
    "            aug_grad = jnp.concatenate([jnp.array([g_t]),g_p])\n",
    "            opt_state = opt_update(step,aug_grad,opt_state)\n",
    "            return value, opt_state\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            value, opt_state = step_fun(step, opt_state,psi,psi0)\n",
    "            loss_list.append(value)\n",
    "            print('step {0}: loss is {1}'.format(step,value))\n",
    "    \n",
    "    plt.plot([x for x in range(num_steps*batch_size)],loss_list)\n",
    "    return get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_gate_U = tensor([sigmax(),sigmax(),sigmax(),sigmax()]).full() # Take $\\otimes simga_z^i$ as test unitary gate\n",
    "# print(Test_gate_U.full())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "Cpfn-q-UGODt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss is 65.23636380271425\n",
      "step 1: loss is 295.61310276716705\n",
      "step 2: loss is 143263.43638440795\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    t1 = 10.\n",
    "    Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "    flat_params = initial(keyinit)\n",
    "    data_batch= generate_batch(Test_gate_U,keypsi,batch_size=1)\n",
    "    res_state = q_optimize(data_batch,Loss_vector,flat_params,t1,learning_rate=1,mass=1.,num_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check different back propagation methods time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 303 ms, sys: 27.5 ms, total: 330 ms\n",
      "Wall time: 336 ms\n",
      "(84,)\n",
      "CPU times: user 1min 10s, sys: 13.2 s, total: 1min 23s\n",
      "Wall time: 1min 24s\n",
      "6.091540803541452\n"
     ]
    }
   ],
   "source": [
    "psi,psi0 = data_batch\n",
    "t1 =10.\n",
    "%time res = forward(t1,psi[:,0],flat_params,method='matrix')\n",
    "%time loss_val, gd = grad_all(res,psi0[:,0],flat_params,t1,Loss_vector)  # grad_all(psi1,flat_p,t1,A,psi0)\n",
    "print(loss_val)\n",
    "\n",
    "psi,psi0 = generate_batch(Test_gate_U,keypsi,batch_size=1)\n",
    "t1 =2.\n",
    "%time res = forward(t1,psi[:,0],flat_params,method='matrix')\n",
    "%time loss_val, gd = grad_all(res,psi0[:,0],flat_params,t1,Loss_vector)  # grad_all(psi1,flat_p,t1,A,psi0)\n",
    "\n",
    "psi,psi0 = generate_batch(Test_gate_U,keypsi,batch_size=1)\n",
    "t1 = 10.\n",
    "%time val,gd_jax=jax_fwd(psi[:,0],t1,flat_params,psi0[:,0],Loss_vector)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check our batch correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypsi, keyinit = random.split(key)\n",
    "psi_batch,psi0_batch = generate_batch(Test_gate_U,keypsi)\n",
    "mode_psi = jnp.einsum('ijk,ijk->jk',psi_batch,psi_batch.conj())\n",
    "mode_psi0 = jnp.einsum('ijk,ijk->jk',psi0_batch,psi0_batch.conj())\n",
    "\n",
    "print(mode_psi)\n",
    "# print(mode_psi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we check two different forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.008947887506927-5.001847503832613e-18j)\n",
      "64.74345324743116\n",
      "64.76980975071014\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,0,0]\n",
    "psi0 = psi0_batch[:,0,0]\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "\n",
    "res_sci = forward(t1,psi,flat_params,func_method='matrix')\n",
    "print(jnp.dot(res_sci.conj(),res_sci))\n",
    "loss_sci = loss(psi,flat_params,t1,Loss_vector,psi0)\n",
    "loss_jax= jax_fwd(psi,t1,flat_params,psi0,Loss_vector)\n",
    "\n",
    "print(loss_sci) # not accurate\n",
    "print(loss_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we check with back propagation with `check_grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,0,0]\n",
    "psi0 = psi0_batch[:,0,0]\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "\n",
    "jax_grad_check = lambda t1,p: jax_fwd(psi,t1,p,psi0,Loss_vector)\n",
    "check_grads(jax_grad_check,(t1,flat_params),order=1,modes='rev') # No error thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n",
      "CPU times: user 34.2 ms, sys: 121 Âµs, total: 34.3 ms\n",
      "Wall time: 34.4 ms\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,:,1]\n",
    "psi0 = psi0_batch[:,:,1]\n",
    "print(psi.shape)\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "\n",
    "%time value,grads = jax.value_and_grad(jax_fwd,(1,2))(psi,t1,flat_params,psi0,Loss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "285px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
