{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8-yYZglGODU",
    "outputId": "6a9ebd71-7435-4a97-db10-5f2fcf195af3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "import jax\n",
    "from jax import jit,vmap,grad\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import vjp\n",
    "from qutip import *\n",
    "from jax.experimental import ode,optimizers\n",
    "from jax.experimental.ode import odeint\n",
    "\n",
    "from jax.config import config   \n",
    "config.update(\"jax_enable_x64\", True) # Force Jax use float64 as default Float dtype\n",
    "\n",
    "from jax.tree_util import tree_unflatten,tree_flatten\n",
    "from jax.test_util import check_grads # import jax implemented method to check the grads\n",
    "\n",
    "\n",
    "N = 4 # set number of qubits as a global parameter\n",
    "N1 = 10 #fourier series size\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1cKxLXnVGODZ"
   },
   "outputs": [],
   "source": [
    "def normalize(psi):\n",
    "    '''\n",
    "    Normalize a given wavefunction\n",
    "    '''\n",
    "    return psi/jnp.linalg.norm(psi,axis=0,keepdims=True)\n",
    "\n",
    "def generate_batch(U,key,batch_size=10,batch_num=3,*args):\n",
    "    '''\n",
    "    To generate a bach of data point given a unitary gate\n",
    "    '''\n",
    "    key_re , key_im = random.split(key)\n",
    "    psi_batch = random.normal(key_re,shape=(2**N,batch_size,batch_num)) + 1j*random.normal(key_im,shape=(2**N,batch_size,batch_num))\n",
    "    psi0_batch = jnp.einsum('ijk,li->ljk',psi_batch,U)\n",
    "    #      initial psi          training data psi0\n",
    "    return normalize(psi_batch), normalize(psi0_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPSCTqnOGODc"
   },
   "source": [
    "To perceed neural network, an ODE solver is needed. In our case is `scipy.integrate.solve_ivp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "id": "gyjrFww7GODc"
   },
   "outputs": [],
   "source": [
    "def forward(t1,psi0,flat_p,func_method='matrix'):\n",
    "    '''\n",
    "    solver for Schrodinger equation with initial condition of psi0 and Hamiltonian in the interval of [0,t1]\n",
    "    use mathod of scipy\n",
    "    '''\n",
    "    if func_method == 'function':\n",
    "        Hz = lambda t,x: -1.0j*H(t,x,flat_p,t1)\n",
    "    elif func_method == 'matrix':\n",
    "        def Hz(t,x,*args): \n",
    "            t1,flat_p, = args\n",
    "            return -1.0j*Hmat(t,x,flat_p,t1)\n",
    "    else:\n",
    "        raise NameError(\"wrong solver\")\n",
    "    \n",
    "    sol = solve_ivp(Hz,[0,t1],psi0,t_eval=[0,t1],args=[t1,flat_p],method='RK45') # There are something wrong with the precision control in solve_ivp\n",
    "    return sol.y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "J649wAPpGODf"
   },
   "outputs": [],
   "source": [
    "def initial(key1 = None):\n",
    "    '''\n",
    "    Return randomized parameters\n",
    "    '''\n",
    "    if (key1).all() == None:\n",
    "        return np.random.normal(size=(N1*(N+1)+1,))\n",
    "    else:\n",
    "        return random.normal(key1,shape=(N1*(N+1)+1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "id": "QVz6THRdGODj"
   },
   "outputs": [],
   "source": [
    "def n(i,x):\n",
    "    '''\n",
    "    number operator for n_i (x)\n",
    "    '''\n",
    "    return (x&2**i)/2**i\n",
    "\n",
    "def unpackp(x):\n",
    "    return jnp.array(x[:N1]) , jnp.array([x[(i+1)*N1:(i+2)*N1] for i in range(N)]), jnp.array(x[-1], dtype=jnp.float32)\n",
    "\n",
    "def flatten(x):\n",
    "    return 0 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPT96vU6ZZ4r"
   },
   "source": [
    "Our Hamiltonian is $$\\begin{aligned}\n",
    "\\frac{H}{\\hbar}=& \\frac{\\Omega(t)}{2} \\sum_{i=1}^{N} \\sigma_{x}^{(i)}-\\sum_{i=1}^{N} \\Delta_{i}(t) n_{i} \\\\\n",
    "&+\\sum_{i<j} \\frac{V}{|i-j|^{6}} n_{i} n_{j}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "zBCGwRkgGODl"
   },
   "outputs": [],
   "source": [
    "def H(t,psi,flat_p,t1):\n",
    "    '''\n",
    "    Using xor to implement off diagonal element\n",
    "    '''\n",
    "    D, = jnp.shape(psi)\n",
    "    res = jnp.zeros(D,dtype=jnp.complex64)\n",
    "\n",
    "    w = 2*jnp.pi/t1\n",
    "\n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    omega = jnp.sum(jnp.array([u_omega[i]*jnp.sin(w*(i+1)*t) for i in range(N1)]))\n",
    "    delta = [jnp.sum(jnp.array([u_d[j,i]*jnp.sin(w*(i+1)*t) for i in range(N1)])) for j in range(N)]\n",
    "    \n",
    "    for x in range(D):\n",
    "        Ci = psi[x]\n",
    "        diag = - jnp.sum(jnp.array([ delta[i]*n(i,x) for i in range(N)]))\\\n",
    "        + jnp.sum(jnp.array([[V*n(i,x)*n(j,x)/jnp.abs(i-j)**6 if i<j else 0 for i in range(N)] for j in range(N)])) #diagonal part of hamiltonian\n",
    "        res = jax.ops.index_add(res, (x,), diag*Ci)\n",
    "        cast = jnp.array([x^2**i for i in range(N)]) # ^ is for xor, to calculate the flip operation\n",
    "        res = jax.ops.index_add(res, cast, omega*Ci/2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed we use dense matrix to represent Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     5,
     23
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "n0 = jnp.array([jnp.diag(jnp.array([n(i,x) for x in range(2**N)])) for i in range(N)]) #number operator matrix form n0[i] = n_i\n",
    "\n",
    "print(n0.shape)\n",
    "#print(jnp.matmul(n0[1],n0[2]))\n",
    "\n",
    "def H_independent():\n",
    "    '''\n",
    "    Time independent part of Hamiltonian\n",
    "    '''\n",
    "    res = jnp.zeros((2**N,2**N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if j<= i: continue\n",
    "            params = 1/np.abs(i-j)**6\n",
    "            res += params*(jnp.dot(n0[i],n0[j]))\n",
    "    return res\n",
    "H1 = H_independent()\n",
    "# print(jnp.trace(H1))\n",
    "\n",
    "f = lambda x,y,i : 1 if y==x^2**i else 0\n",
    "H2 = sum([jnp.array([[f(x,y,i) for x in range(2**N)] for y in range(2**N)]) for i in range(N)]) #checked\n",
    "\n",
    "@jit\n",
    "def Hmat(t,psi,flat_p,t1):\n",
    "    '''\n",
    "    Using dense matrix to represent Hamiltonian\n",
    "    '''\n",
    "    D, = jnp.shape(psi)\n",
    "    res = jnp.zeros(D,dtype=jnp.complex64)\n",
    "    \n",
    "    w = 2*jnp.pi/t1\n",
    "    \n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    \n",
    "    ft =jnp.array([jnp.sin(w*(i+1)*t) for i in range(N1)])\n",
    "\n",
    "    omega = jnp.dot(u_omega,ft)\n",
    "    delta = u_d@ft\n",
    "    \n",
    "    return (V*H1 + 0.5* omega* H2 - jnp.einsum('i,ijk->jk',delta,n0))@psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "id": "Voa8tv0HGODo"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(psi,flat_p,t1,A,psi0):\n",
    "    '''\n",
    "    define the loss function, which is a pure function\n",
    "    '''\n",
    "    omega = 2*jnp.pi/t1\n",
    "    l1 = 1 - jnp.abs(jnp.dot(jnp.conjugate(psi),psi0))**2 # Overlap\n",
    "    \n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    freqeuncy = jnp.array([i*omega**2 for i in range(N1)])\n",
    "    l2 = 0.5*jnp.dot(jnp.conjugate(freqeuncy),u_omega**2)\n",
    "#     l2 = 0.5*jnp.sum(jnp.array([ (i*omega*flat_p[i])**2 for i in range(N1)]))\n",
    "    l3 = 0.5*jnp.sum(jnp.dot(u_d**2,jnp.conjugate(freqeuncy)))\n",
    "#     l3 = 0.5*jnp.sum(jnp.array([ (i*omega*flat_p[i+N1])**2 for i in range(N1)]))\n",
    "    l4 = 0.5*V**2\n",
    "    \n",
    "    l5 = t1**2 # Punish total time\n",
    "    \n",
    "    return A[0]*l1+A[1]*l2+A[2]*l3+A[3]*l4+A[4]*l5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "ak3mbk0YGODq"
   },
   "outputs": [],
   "source": [
    "def grad_all(psi1,psi0,flat_p,t1,A,method='matrix'):\n",
    "    '''\n",
    "    gradient with adjoint method\n",
    "    return value and grad\n",
    "    '''\n",
    "    if method=='matrix':\n",
    "        H_evol = Hmat\n",
    "    elif method == 'function':\n",
    "        H_evol = H\n",
    "    else:\n",
    "        raise NameError('Wrong dynamics')\n",
    "        \n",
    "    D, = jnp.shape(psi1)\n",
    "    \n",
    "    loss_val, a0 = jax.value_and_grad(loss, argnums=0)(psi1,flat_p,t1,A,psi0)\n",
    "\n",
    "    pLt1 = jnp.array([jnp.dot(a0,H_evol(t1,psi1,flat_p,t1))])\n",
    "    \n",
    "    aug_state = jnp.concatenate([psi1, a0, -pLt1, jnp.zeros(((N+1)*N1+1,))])\n",
    "    print(aug_state.shape)\n",
    "    def unpack(x):\n",
    "              # z , vjp_z   , vjp_t , vjp_args\n",
    "        return x[:D], x[D:2*D], x[2*D], x[2*D+1:]\n",
    "    \n",
    "    def wrap_vjp(augment_state,t,parameters):\n",
    "        Hz = lambda z : H_evol(t,z,parameters,t1)\n",
    "        Ht = lambda t0 : H_evol(t0,augment_state[0],parameters,t1)\n",
    "        Hp = lambda p : H_evol(t,augment_state[0],p,t1)\n",
    "        _, vjp_funz = vjp(Hz, augment_state[0])\n",
    "        _, vjp_funt = vjp(Ht, t)\n",
    "        _, vjp_funp = vjp(Hp, flat_p)\n",
    "        \n",
    "        # vjp_funz(augment_state[1]) is a tuple\n",
    "#         print(vjp_funz(augment_state[1])[0].shape,vjp_funp(augment_state[1])[0].shape,vjp_funt(augment_state[1])[0].shape)\n",
    "        return [- vjp_funz(augment_state[1])[0],- vjp_funp(augment_state[1])[0],- vjp_funt(augment_state[1])[0]]\n",
    "    \n",
    "    def aug_dynamics(t,augment_state,*args):\n",
    "        parameters, = args\n",
    "        unpacked_aug_state = unpack(augment_state)\n",
    "\n",
    "        vjps = wrap_vjp(unpacked_aug_state,t,parameters)\n",
    "        aug_grad = jnp.concatenate([H_evol(t,unpacked_aug_state[0],parameters,t1), vjps[0], vjps[1], jnp.array([vjps[2]])])\n",
    "\n",
    "        return aug_grad\n",
    "\n",
    "    aug_state0 = solve_ivp(aug_dynamics,[t1,0],aug_state,t_eval=[t1,0],args=(flat_p,)) # Couldn't use jit for this, since it has no fixed memory.\n",
    "    return loss_val, aug_state0.y[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJSUd71THAQq"
   },
   "source": [
    "Our Hamiltonian is \n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{H}{\\hbar}=& \\frac{\\Omega(t)}{2} \\sum_{i=1}^{N} \\sigma_{x}^{(i)}-\\sum_{i=1}^{N} \\Delta_{i}(t) n_{i} \\\\\n",
    "&+\\sum_{i<j} \\frac{V}{|i-j|^{6}} n_{i} n_{j}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "tSyOBEQjtKyS"
   },
   "outputs": [],
   "source": [
    "def check_with_qutip(psi0,flat_p,t1):\n",
    "    q_psi = Qobj(psi0,dims=[[2,2,2,2], [1 ,1,1,1]])\n",
    "\n",
    "    t = np.linspace(0,t1,100)\n",
    "    w = 2*jnp.pi/t1\n",
    "\n",
    "    u_omega, u_d, V = unpackp(flat_p)\n",
    "    \n",
    "    ft = lambda t: jnp.array([jnp.sin(w*(i+1)*t) for i in range(N1)])\n",
    "    omega = lambda t,args: jnp.dot(u_omega,ft(t))\n",
    "    delta = lambda t,args: -u_d@ft(t)\n",
    "    delta_func = [lambda t,args: delta(t,args)[i] for i in range(N1)]\n",
    "#     omega = lambda t,args : jnp.sum(jnp.array([u_omega[i]*jnp.sin(w*(i+1)*t) for i in range(N1)]))\n",
    "#     delta_func = [lambda t,args : - jnp.sum(jnp.array([u_d[j,i]*jnp.sin(w*(i+1)*t) for i in range(N1)])) for j in range(N)]\n",
    "    h = []\n",
    "\n",
    "    si = qeye(2)\n",
    "    sx = sigmax()\n",
    "    sy = sigmay()\n",
    "    sz = sigmaz()\n",
    "    ni = num(2)\n",
    "    # time independent part\n",
    "    h_list = []\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if j<= i: continue\n",
    "            param = V/np.abs(i-j)**6\n",
    "            op_list=[si for _ in range(N)]\n",
    "            op_list[i] = ni\n",
    "            op_list[j] = ni\n",
    "            h_list.append(param*tensor(op_list))\n",
    "    H0 = sum(h_list)\n",
    "#     print(H0.tr()/V)\n",
    "    h.append(H0)\n",
    "    \n",
    "    #time dependent part\n",
    "    sx_list = []\n",
    "\n",
    "    for n in range(N):\n",
    "        op_list = []\n",
    "        for m in range(N):\n",
    "            op_list.append(si)\n",
    "\n",
    "        op_list[n] = sx\n",
    "        sx_list.append(tensor(op_list))\n",
    "\n",
    "    # construct the hamiltonian\n",
    "    H1 = sum([0.5 * sx_list[n] for n in range(N)])\n",
    "    h.append([H1,omega])\n",
    "\n",
    "    for i in range(N):\n",
    "        n_list = [si for _ in range(N)]\n",
    "        n_list[i] = ni\n",
    "        H2= tensor(n_list)\n",
    "        h.append([H2,delta_func[i]])\n",
    "    output = mesolve(h, q_psi, t, progress_bar = True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to use `jax.experimental.ode.odeint` to autograd our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def jax_fwd(psi_init,t1,flat_p,psi0,Loss_vec):\n",
    "    '''\n",
    "    Implement forward odeint with jax.experimental.ode.odeint\n",
    "    \n",
    "    psi_init: a batch of \n",
    "    \n",
    "    return loss\n",
    "    '''\n",
    "    D,batch_size = jnp.shape(psi_init)\n",
    "    \n",
    "    def func(y,t,*args):\n",
    "        t1,flat_p, = args\n",
    "        return -1.0j*Hmat(t,y,flat_p,t1)\n",
    "    t_set = jnp.linspace(0.,t1,5)\n",
    "    \n",
    "    res_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        res = odeint(func,psi_init[:,i],t_set,t1,flat_p,rtol=1.4e-10, atol=1.4e-10)\n",
    "        psi_final = res[-1,:]\n",
    "        res_list.append(psi_final)\n",
    "\n",
    "    return jnp.sum(jnp.array([loss(res_list[i],flat_p,t1,Loss_vec,psi0[:,i]) for i in range(batch_size)]))\n",
    "\n",
    "def jax_fwd_psi(psi_init,t1,flat_p,psi0,Loss_vec):\n",
    "    '''\n",
    "    Same implement as jax_fwd\n",
    "    Return wave fucntion, not batch\n",
    "    '''\n",
    "    \n",
    "    D, = jnp.shape(psi_init)\n",
    "    \n",
    "    def func(y,t,*args):\n",
    "        t1,flat_p, = args\n",
    "        return -1.0j*Hmat(t,y,flat_p,t1)\n",
    "    t_set = jnp.linspace(0.,t1,5)\n",
    "    \n",
    "    res = odeint(func,psi_init,t_set,t1,flat_p,rtol=1.4e-10, atol=1.4e-10)\n",
    "    psi_final = res[-1,:]\n",
    "\n",
    "    return psi_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def q_optimize(data_batch,Loss_vector,flat_params,t1,learning_rate=1.,mass=0.9,num_steps=5):\n",
    "    '''\n",
    "    Train the neural ode over a given databatch\n",
    "    '''\n",
    "    psi_batch,psi0_batch = data_batch # we try with single wavefunction for instance\n",
    "    \n",
    "    D,batch_size,batch_num = jnp.shape(psi_batch)\n",
    "    \n",
    "#     opt_init, opt_update, get_params = optimizers.momentum(learning_rate,mass) # Use momentum as optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.adam(learning_rate) # Use adam optimizer\n",
    "    loss_list = []\n",
    "    def unpack(x):\n",
    "          # t , args\n",
    "        return x[0], x[1:]\n",
    "\n",
    "    def step_fun(step, opt_state,psi,psi0):\n",
    "#         value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state))\n",
    "        aug_params = get_params(opt_state)\n",
    "\n",
    "        t1,flat_params = unpack(aug_params)\n",
    "#             res = forward(t1,psi,flat_params,method='matrix') # Forward with scipy RK45\n",
    "#             value, grads = grad_all(res,psi0,flat_params,t1,Loss_vector) # grad with self \n",
    "#             opt_state = opt_update(step, jnp.real(grads[-L:]), opt_state)\n",
    "        value,grads = jax.value_and_grad(jax_fwd,(1,2))(psi,t1,flat_params,psi0,Loss_vector)\n",
    "        g_t, g_p = grads\n",
    "        aug_grad = jnp.concatenate([jnp.array([g_t]),g_p])\n",
    "        opt_state = opt_update(step,aug_grad,opt_state)\n",
    "        return value, opt_state\n",
    "    \n",
    "    aug_params = jnp.concatenate([jnp.array([t1]),flat_params])\n",
    "    opt_state = opt_init(aug_params)\n",
    "    \n",
    "    # optimize with mini_batch\n",
    "    for step in range(num_steps):\n",
    "        for i in range(batch_num):\n",
    "            psi,psi0 = psi_batch[:,:,i],psi0_batch[:,:,i]\n",
    "            value, opt_state = step_fun(step, opt_state,psi,psi0)\n",
    "            loss_list.append(value)\n",
    "            print('step {0} with batch {2}: loss is {1}'.format(step,value,i))\n",
    "    plt.plot([x for x in range(num_steps*batch_num)],loss_list)\n",
    "    return get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_gate_U = tensor([sigmax(),sigmax(),sigmax(),sigmax()]).full() # Take $\\otimes simga_z^i$ as test unitary gate\n",
    "# print(Test_gate_U.full())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "id": "Cpfn-q-UGODt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 with batch 0: loss is 2977.922088624734\n",
      "step 0 with batch 1: loss is 2763.242316290945\n",
      "step 0 with batch 2: loss is 2611.124571817826\n",
      "step 1 with batch 0: loss is 2610.090185756202\n",
      "step 1 with batch 1: loss is 2600.846688145419\n",
      "step 1 with batch 2: loss is 2544.4700982246586\n",
      "step 2 with batch 0: loss is 2418.4447103394205\n",
      "step 2 with batch 1: loss is 2326.7152577813704\n",
      "step 2 with batch 2: loss is 2262.998737830285\n",
      "step 3 with batch 0: loss is 2227.0664918051743\n",
      "step 3 with batch 1: loss is 2182.837334478275\n",
      "step 3 with batch 2: loss is 2191.1998875587306\n",
      "step 4 with batch 0: loss is 2185.6287045158497\n",
      "step 4 with batch 1: loss is 2222.7046196621727\n",
      "step 4 with batch 2: loss is 2201.864453979477\n",
      "step 5 with batch 0: loss is 2171.5184565076115\n",
      "step 5 with batch 1: loss is 2122.1955047854062\n",
      "step 5 with batch 2: loss is 2144.747917320007\n",
      "step 6 with batch 0: loss is 2140.1345651068677\n",
      "step 6 with batch 1: loss is 2131.994013995772\n",
      "step 6 with batch 2: loss is 2145.6453177742183\n",
      "step 7 with batch 0: loss is 2118.2420878007815\n",
      "step 7 with batch 1: loss is 2126.978381139763\n",
      "step 7 with batch 2: loss is 2071.4339663525243\n",
      "step 8 with batch 0: loss is 2027.8955646530042\n",
      "step 8 with batch 1: loss is 2028.7920441328654\n",
      "step 8 with batch 2: loss is 2049.9257041253873\n",
      "step 9 with batch 0: loss is 1913.886921662201\n",
      "step 9 with batch 1: loss is 2022.5607655923677\n",
      "step 9 with batch 2: loss is 1858.0282299054854\n",
      "step 10 with batch 0: loss is 2087.0531751602657\n",
      "step 10 with batch 1: loss is 1995.6256579360156\n",
      "step 10 with batch 2: loss is 1975.383679744703\n",
      "step 11 with batch 0: loss is 1929.645686867453\n",
      "step 11 with batch 1: loss is 1949.6547070026597\n",
      "step 11 with batch 2: loss is 1885.704247561232\n",
      "step 12 with batch 0: loss is 1867.6157483681823\n",
      "step 12 with batch 1: loss is 1896.5217628690602\n",
      "step 12 with batch 2: loss is 2034.9681940051496\n",
      "step 13 with batch 0: loss is 2047.1795424591346\n",
      "step 13 with batch 1: loss is 2103.2261979000205\n",
      "step 13 with batch 2: loss is 2084.75240347759\n",
      "step 14 with batch 0: loss is 1990.1667083480577\n",
      "step 14 with batch 1: loss is 1949.0383654497841\n",
      "step 14 with batch 2: loss is 1953.5420107689345\n",
      "step 15 with batch 0: loss is 1941.283492117242\n",
      "step 15 with batch 1: loss is 1934.227974431166\n",
      "step 15 with batch 2: loss is 1973.0400777262782\n",
      "step 16 with batch 0: loss is 1899.4727414285258\n",
      "step 16 with batch 1: loss is 1847.3132427804394\n",
      "step 16 with batch 2: loss is 1921.2292117687336\n",
      "step 17 with batch 0: loss is 1862.1048679532596\n",
      "step 17 with batch 1: loss is 1885.8356192256692\n",
      "step 17 with batch 2: loss is 1897.8857891950283\n",
      "step 18 with batch 0: loss is 1839.9418186586454\n",
      "step 18 with batch 1: loss is 1842.4942655547416\n",
      "step 18 with batch 2: loss is 1890.7146385870808\n",
      "step 19 with batch 0: loss is 1784.7457907866726\n",
      "step 19 with batch 1: loss is 1813.594406186917\n",
      "step 19 with batch 2: loss is 1909.7423216012544\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxzElEQVR4nO3deXiU5bn48e+dnaxkJysJEHbCFgFFFEEFlRatVWmtcqwt1aN1OVorrVd/p4vVWvVUbeFIXdDjVqtWLRWXIijKZkC2sAayEEjISvY9z++PeYMBJmRCJplM5v5c11x553mXuZ9LvOedZ3vFGINSSinP4OXqAJRSSvUdTfpKKeVBNOkrpZQH0aSvlFIeRJO+Ukp5EB9XB9CVqKgok5KS4uowlFLKrWzdurXUGBN9enm/T/opKSlkZma6OgyllHIrIpJnr1ybd5RSyoN0mfRFJEBEtojIDhHJEpFfW+URIvKJiBy0/oZ3OGepiGSLyH4RmdehfKqI7LL2PS0i0jvVUkopZY8jd/qNwBxjzERgEjBfRGYADwJrjDFpwBrrPSIyFlgEjAPmA8tExNu61nJgCZBmveY7rypKKaW60mXSNzY11ltf62WAhcBLVvlLwNXW9kLgDWNMozEmB8gGpolIHBBqjNlobGs/vNzhHKWUUn3AoTZ9EfEWke1AMfCJMWYzEGuMKQSw/sZYhycARzqcXmCVJVjbp5crpZTqIw4lfWNMqzFmEpCI7a59/FkOt9dOb85SfuYFRJaISKaIZJaUlDgSolJKKQd0a/SOMeYEsA5bW/xxq8kG62+xdVgBkNThtETgmFWeaKfc3uesMMZkGGMyoqPPGGaqlFLqHDkyeidaRAZb24OAS4F9wPvAYuuwxcB71vb7wCIR8ReRVGwdtlusJqBqEZlhjdq5ucM5Tvfyxlze32H3O0UppTyWI5Oz4oCXrBE4XsCbxphVIrIReFNEbgXygesAjDFZIvImsAdoAe4wxrRa17odWAkMAlZbr17xxpYjRIf48+2J8b31EUop5Xa6TPrGmJ3AZDvlZcDcTs55GHjYTnkmcLb+AKdJjQpi97HKvvgopZRyGwN2Rm5KVCAFFfU0t7a5OhSllOo3BmzST40KprXNcKS8ztWhKKVUvzGAk34gALlltS6ORCml+o8Bm/RTIoMAOFyiSV8ppdoN2KQfEeRHSICP3ukrpVQHAzbpiwipUUHklmqbvlJKtRuwSR9sTTw5pXqnr5RS7QZ20o8K4lhlPQ3NrV0frJRSHmBAJ/3UqECMQYdtKqWUZYAn/WAAbeJRSinLwE761rBNHcGjlFI2AzrphwX6Eh7oq3f6SillGdBJH2yduZr0lVLKZsAn/dRIHauvlFLtBnzST4kKoqiqgfomHbaplFIekfRBO3OVUgo8IOkPa0/62q6vlFIDP+m33+nn6J2+UkoN/KQf7O9DVLC/3ukrpRQekPTBthyDDttUSikPSfq21TZ12KZSSnlG0o8KorSmkeqGZleHopRSLuURST/V6szNK9O7faWUZ/OIpN/+vFxt11dKeTrPSPpRgYCO1VdKqS6TvogkichaEdkrIlkicrdVPklENonIdhHJFJFpHc5ZKiLZIrJfROZ1KJ8qIrusfU+LiPROtU4V6OfDkNAAHauvlPJ4jtzptwD3GWPGADOAO0RkLPAY8GtjzCTgV9Z7rH2LgHHAfGCZiHhb11oOLAHSrNd851Xl7FKiAvVOXynl8bpM+saYQmPMNmu7GtgLJAAGCLUOCwOOWdsLgTeMMY3GmBwgG5gmInFAqDFmozHGAC8DVzuzMmeTqkssK6UUPt05WERSgMnAZuAe4CMReRzbl8cF1mEJwKYOpxVYZc3W9unl9j5nCbZfBCQnJ3cnxE6lRAZRUddMZV0zYYG+TrmmUkq5G4c7ckUkGHgbuMcYUwXcDtxrjEkC7gWebz/UzunmLOVnFhqzwhiTYYzJiI6OdjTEs9I1eJRSysGkLyK+2BL+q8aYd6zixUD79t+B9o7cAiCpw+mJ2Jp+Cqzt08v7RKqutqmUUg6N3hFsd/F7jTFPdth1DLjY2p4DHLS23wcWiYi/iKRi67DdYowpBKpFZIZ1zZuB95xUjy4lRwQiomP1lVKezZE2/ZnATcAuEdlulf0C+DHwlIj4AA1YbfDGmCwReRPYg23kzx3GmPbHVt0OrAQGAautV58I8PUmPmyQPkxFKeXRukz6xpgvsN8eDzC1k3MeBh62U54JjO9OgM6kI3iUUp7OI2bkths9JIT9RdU0t7a5OhSllHIJj0r6ExLDaGxp48DxaleHopRSLuFRSX9i4mAAdhVUujYQpZRyEY9K+kMjAwkJ8GGHJn2llIfyqKQvIqQnhrHr6AlXh6KUUi7hUUkfID1xMPsKq2lobu36YKWUGmA8L+knhNHSZthXpJ25SinP43lJP2kwALsKTrg0DqWUcgWPS/rxYQFEBvlpZ65SyiN5XNIXESYkhumwTaWUR/K4pA+2ztyDxdXUNbW4OhSllOpTnpn0E8JoM5B1rMrVoSilVJ/yzKSfGAbATm3iUUp5GI9M+jGhAQwJDWCnjuBRSnkYj0z6YLvb185cpZSn8eikf7i0lsr6ZleHopRSfcZjk/4Ea8XNrKN6t6+U8hwem/TTE6zOXE36SikP4rFJPzzIj6SIQdqZq5TyKB6b9ME2SUuHbSqlPIlnJ/2EMAoq6imraXR1KEop1Sc8OulPsCZp7dJ2faWUh/DspG915up4faWUp/DopB8S4Muw6CBdZlkp5TF8ujpARJKAl4EhQBuwwhjzlLXvp8CdQAvwL2PMA1b5UuBWoBW4yxjzkVU+FVgJDAI+AO42xhgn16lbJiYO5qOsIm5d+RX+vl4E+Hjj7+tF6CBfZg6PYvqwCPx9vF0ZolJKOU2XSR9bQr/PGLNNREKArSLyCRALLATSjTGNIhIDICJjgUXAOCAe+LeIjDTGtALLgSXAJmxJfz6w2tmV6o7rMhI5Ul5HUVUDjS1tNDS30tjSRmVdM89+dphAP29mpUUxd3Qss0dHExMS4MpwlVKqR7pM+saYQqDQ2q4Wkb1AAvBj4FFjTKO1r9g6ZSHwhlWeIyLZwDQRyQVCjTEbAUTkZeBqXJz0LxgexQW3R51RXt/UysbDpazZW8yn+4r5KOs4Pl7CC/9xHheNjHZBpEop1XPdatMXkRRgMrAZGAnMEpHNIvKZiJxnHZYAHOlwWoFVlmBtn15u73OWiEimiGSWlJR0J0SnGeTnzZzRsTx8zQQ2PDiH1XfPIikikN+u2kNLa5tLYlJKqZ5yOOmLSDDwNnCPMaYK26+EcGAG8DPgTRERQOycbs5SfmahMSuMMRnGmIzoaNffVYsIY+JC+fn80RwsruHvWwu6Pkkppfohh5K+iPhiS/ivGmPesYoLgHeMzRZsnbxRVnlSh9MTgWNWeaKdcrcxb1wsGUPDeeLjA9Q26qMWlVLup8ukb929Pw/sNcY82WHXu8Ac65iRgB9QCrwPLBIRfxFJBdKALVbfQLWIzLCueTPwnjMr09tEhF9cNYbSmkZWfH7Y1eEopVS3OXKnPxO4CZgjItut15XAC8AwEdkNvAEstu76s4A3gT3Ah8Ad1sgdgNuB54Bs4BAu7sQ9F1OSw7kqPY4Vnx+muKrB1eEopVS3iIuHyXcpIyPDZGZmujqMU+SX1TH3yXVcOyWRR69Nd3U4Sil1BhHZaozJOL3co2fknqvkyEBumpHCm5lHOHC82tXhKKWUwzTpn6OfzhlBsL8Pj3yw19WhKKWUwzTpn6PwID/unDOCtftL2HCo1NXhKKWUQzTp98DN56cQ4u/Dqp2Frg5FKaUcokm/BwJ8vclICWfT4TJXh6KUUg7RpN9D04dFcrikluJqHb6plOr/NOn30IxhkQBsySl3cSRKKdU1Tfo9ND4+lCA/bzYf1qSvlOr/NOn3kI+3F1NTIrRdXynlFjTpO8H01AgOFtdQVtPo6lCUUuqsNOk7gbbrK6XchSZ9J0hPDGOQrzebNekrpfo5TfpO4OvtxdShOl5fKdX/adJ3kumpEewrqqaitsnVoSilVKc06TvJ9PZ2/Vxt4lFK9V+a9J1kYlIY/j5eOl5fKdWvadJ3En8fb6Ykh7M5R9v1lVL9lyZ9J5o+LII9hVVU1je7OhSllLJLk74TTU+NxBj4SoduKqX6KU36TjQ5eTB+3l7axKOU6rc06TtRgK83k5IH6yQtpVS/pUnfyWakRrD7aCXVDdqur5TqfzTpO9n0YZG0GcjMrXB1KEopdQZN+k42JTkcX2/RJRmUUv1Sl0lfRJJEZK2I7BWRLBG5+7T994uIEZGoDmVLRSRbRPaLyLwO5VNFZJe172kREedWx/UG+XkzLTWCj/ccxxjj6nCUUuoUjtzptwD3GWPGADOAO0RkLNi+EIDLgPz2g619i4BxwHxgmYh4W7uXA0uANOs130n16FcWpMeTU1pL1rEqV4eilFKn6DLpG2MKjTHbrO1qYC+QYO3+H+ABoOMt7ULgDWNMozEmB8gGpolIHBBqjNlobLfALwNXO60m/cj8cUPw8RJW7Sx0dShKKXWKbrXpi0gKMBnYLCLfBo4aY3acdlgCcKTD+wKrLMHaPr3c3ucsEZFMEcksKSnpToj9QniQHzNHRLFq5zFt4lFK9SsOJ30RCQbeBu7B1uTzS+BX9g61U2bOUn5moTErjDEZxpiM6OhoR0PsVxakx1FQUc+OgkpXh6KUUic5lPRFxBdbwn/VGPMOMBxIBXaISC6QCGwTkSHY7uCTOpyeCByzyhPtlA9Il48bgp+3F6t2DNgqKqXckCOjdwR4HthrjHkSwBizyxgTY4xJMcakYEvoU4wxRcD7wCIR8ReRVGwdtluMMYVAtYjMsK55M/Be71TL9cIG+XLRyCj+tauQtjZt4lFK9Q+O3OnPBG4C5ojIdut1ZWcHG2OygDeBPcCHwB3GmFZr9+3Ac9g6dw8Bq3sSfH+3ID2ewsoGtuXrRC2lVP/g09UBxpgvsN8e3/GYlNPePww8bOe4TGB890J0X5eOjcXfx4tVOwvJSIlwdThKKaUzcntTsL8Pl4yK4V+7CmnVJh6lVD+gSb+XXZUeR0l1I1/ps3OVUv2AJv1eNndMDIN8vVm1U0fxKKVcT5N+Lwv082HOmBhW7yqipbXN1eEopTycJv0+8K30OMpqm9h0WJt4lFKupUm/D8weFUOQnzf/1IlaSikX06TfBwJ8vbkqPY73dhyltKbR1eEopTyYJv0+8pOLh9PU0sZz63NcHYpSyoNp0u8jw6ODWZAez8sbc6mobXJ1OEopD6VJvw/dOWcEdU2tvPCl3u0rpVxDk34fGhkbwhXjh7Dyy1wq65tdHY5SygNp0u9jd84ZQXVjCy9tyHV1KEopD6RJv4+Niw/j0jExPP9FDjWNLa4ORynlYTTpu8BP56RRWd/MyxtzXR2KUsrDaNJ3gYlJg7loZDTPrc+hrknv9pVSfUeTvovcNWcE5bVNvLY539WhKKU8iCZ9F8lIieCC4ZEsW3eIgoo6V4ejlPIQmvRd6DcLx9Hc2sYPV36lQziVUn1Ck74LjYgJ4dkfTOVwSS23v7KVphZdelkp1bs06bvYBSOiePTadDYcKuMX/9iFMfpYRaVU7+nyweiq9313aiL55XU8veYgyRGB3DU3zdUhKaUGKE36/cS9l6ZRUF7Hk58cICliENdMTnR1SEqpAUibd/oJEeHRa9OZMSyCB97aye6jlU679v6iavYVVTntekop96VJvx/x8/Fi+Y1TCQ/0456/bae+qbXH19xXVMV3ln3JlU+t5w8f7qOxpefXVEq5ry6TvogkichaEdkrIlkicrdV/kcR2SciO0XkHyIyuMM5S0UkW0T2i8i8DuVTRWSXte9pEZFeqZUbCw/y44nrJ5JdXMOjq/f26FqlNY3cujKTIH8fvjMlkeXrDrHg6S/YceSEc4JVSrkdR+70W4D7jDFjgBnAHSIyFvgEGG+MSQcOAEsBrH2LgHHAfGCZiHhb11oOLAHSrNd8J9ZlwJiVFs0PZ6by0sY81u4vPqdrNDS3suTlTMpqG3lucQaPXzeRlbecR3VDC99ZvoHH9K5fKY/UZdI3xhQaY7ZZ29XAXiDBGPOxMaZ94ZhNQHvP40LgDWNMozEmB8gGpolIHBBqjNlobOMSXwaudm51Bo4H5o9iZGwwD7y1k7JuPlfXGMODb+9kW/4Jnrx+EumJgwHbA9o/uvcirpmcwLJ1h/jOsg3dvrZSyr11q01fRFKAycDm03b9EFhtbScARzrsK7DKEqzt08vtfc4SEckUkcySkpLuhDhgBPh686cbJlNZ18zSd84cv2+MoaCizu6CbX9Zm827249x32UjuXJC3Cn7wgb58vh1E1lx01Syi2v43l83UVzd0Kt1UUr1Hw4P2RSRYOBt4B5jTFWH8l9iawJ6tb3IzunmLOVnFhqzAlgBkJGR4bGzlcbGh/KzeaN4+IO9vJl5hLljYvkyu5T1B0v5MruUwsoGRGBoRCCjhoQwekgofj5ePP7xAa6eFM+dc0Z0eu3Lxw3hxVvO49aVmSxasYnXfzyD2NCAPqydUsoVHEr6IuKLLeG/aox5p0P5YmABMNd8cytaACR1OD0ROGaVJ9opV2dx64WprN1fzC/+sZvWt3cBtrv1mSMiuS01ksr6ZvYVVbGvsJqP9xzHGJicPJhHr02nq37yC4ZH8dIPp3HLi1u44dmNvPbjGcQPHtQX1VJKuYh0Ne3fGmHzElBujLmnQ/l84EngYmNMSYfyccBrwDQgHlgDpBljWkXkK+Cn2JqHPgCeMcZ8cLbPz8jIMJmZmedQtYGjqLKBR1bvZWRsCLPSohgXH4a315kJvb6plZzSWkbEBOPn43jL3bb8Cha/sIWwQb68/uMZJEUEOjN8pZQLiMhWY0zGGeUOJP0LgfXALqB9RbBfAE8D/kCZVbbJGHObdc4vsbXzt2BrDlptlWcAK4FB2PoAfmq6CECTft/YWXCCm57fQrC/Dx/cNYuwQF9Xh6SU6oFzTvqupkm/72w/coJrl2/g+oxEHvlOuqvDUUr1QGdJX2fkqpMmJQ3m1gtTeX3LEbbklLs6HKVUL9Ckr05xz6VpJIYPYuk7O3XyllIDkCZ9dYpAPx9+d/V4DpXUsnzdIbvHNDS38vqWfN79+ig7jpzo1lO/qhuaWfllDi9+maPPDlDKBXRpZXWG2aNi+PbEeJatPcSC9HhGxASf3HfweDU/ff1r9hVVn3JOVLAfqVFBTEwczMwRUUxLjSDI/5t/XkfK63jxy1zezDxCTaNtQtmugkoevTa9WyONlFI9ox25yq6S6kYuffIzRsWG8MaSGYjA61uO8JtVWQT5+fCHa9MZGhnI4dJacktrySmtJbu4hp1HK2lqacPHS5iUNJgLhkey/3g1n+w5jpcIC9LjuGVmKusPlvD4xweYOSKS5T+YSmiAjhZSypl09I7qtr99lc/P397FL64czba8E3yYVcSstCieuG4iMZ3M3m1obiUzt4IvD5WyIbuUXUcrCRvky43Th3LT+UNPmfX71tYCHnx7JyNigll5yzSGhH2zr6axhQ3ZpeSX17H4ghR8vfXXgFLdoUlfdZsxhkUrNrE5pxwfL+GB+aP40YXD8LIzMawzNY0t+HoL/j7edvevP1jC7a9sIyTA1pdw4HgNnx0oZmteBc2ttn+b//2tsfzHzFSn1EkpT6FJX52TvLJaHvtoP0tmDWNi0uBe+Yw9x6q4ZeUWjlfZVvwcExfKxSOjuWhkFH/+NJu9hVWsu/8SnTCmVDdo0lf9WnF1A1tzK5g6NPyUpqOsY5UseOYLfnRhKr+8aqwLI1TKvejkLNWvxYQEcMWEuDP6CsbFh3Hd1ERe2pBHXlmti6LzHNnFNTyz5iAn6ppcHYrqJZr0Vb933+Wj8PYS/vDhPleHMqC1tRnu+/sOnvjkAJc8vo7Xt+TT2ta/WwJU92nSV/1ebGgAt108nA92FfFVri4P0VtW7Spkx5ET3HnJCNJiQlj6zi6uWfYlX+dXuDo05USa9JVb+PFFqcSG+vO7f+2lrR/dfTa2tHLt8g2sP+jeT3hraG7lD6v3MSYulHsvG8nffjKDpxZN4nhVA9cs28DSd3bS0trW9YVUv6dJX7mFQD8ffjZvNDuOnOCfO/vPs3dyS+vYmlfByi9zXR1Kj7y8MZejJ+p56KoxeHsJIsLCSQmsuW82/3FBCq9vOcLa/e79xaZsNOkrt/GdyQmMiw/lsQ/309DcPxaDa+9c/vxgCZV1jq9B1J9U1DbxzKfZXDIqmpkjok7ZF+zvwy+vGsPgQF8+2FXoogiVM2nSV27Dy0t46KqxHD1RzxMf73d1OADkl9cB0Nxq+CiryMXRnJun1hyktrGFpVeOsbvf19uLy8fG8u89x3Xl1QFAk75yK+cPj+QHM5L56/qcftGOnl9eR4i/D8kRgf2q2clROaW1vLIpjxvOS2ZkbEinx10xIY7qxha+zC7tw+hUb9Ckr9zOL68cy4iYYO57cwflta4dT55XVkdyZCBXpcex4VAZZTWNLo2nu/6weh/+Pl7ce1naWY+bOTyK0AAf/rXTPX/NqG9o0lduZ5CfN08vmsyJumYeeGunw+vyNzS38vhH+3nhixynxZJfXsfQyEC+lR5Pa5vhQzdq4vkyu5QPs4q47eLhxITYX0CvnZ+PF5eNHcIne4poatFRPO5Mk75yS2PjQ3lg/ij+vfc4r2zO7/L4r/MruPLp9fx5bTZPrTnolGGfrW2Ggoo6kiOCGBMXwrDoIFbt6N+dnQ3Nrby3/Sjf/+smbnxuM/FhAfxo1jCHzr1ywhCqGlrYcEibeNyZJn3ltn44M5VZaVH8btUeDh6vtntMY0srf/xoH9cu30BDUyvfm5ZEZX0zh0trevz5hZX1NLcahkYGIiIsSI9nU04ZxVUNPb62sxVU1PHrf2Yx45E13P3Gdo5U1HH/5SN5986ZDPKzvwLq6S5MiyLY30dH8bg5fXKWclteXsIT10/kij+t5643tvPn70+mpdXQ1NJGY0srlfXN/PGj/ewrqub6jEQeWjCWkupGXt9yhMzcCkbEdN5x6Yj8MtvInaERgQB8Kz2Op9cc5INdhf1uKejbXtnK/qJqLh83hO+dl8wFwyO7tUQ2gL+PN5eOieHjPcd5uLVNn3HgpjTpK7cWExLAY99N59aXMpn7xGdn7I8O8ef5xRnMHRMLQIi/D+GBvmTmVbBoWnKn121rMxyrrCcxPLDTY/Ks4ZpJVtJPiw1hVGwIq3b2r6RfWd/M7qNV/NdlI7lr7tk7bLty5YQ43t1+jE2Hy5iVFu2kCFVf0qSv3N7cMbH8bckMjlXW4+ftjZ+PF/4+Xvj5eDE2PvSURzGKCFOHhrMt7+zryfx96xF++Y/dfPHzOac80aujvLI6fL2F+MGDTpYtSI/jiU8OcOxE/SnlrtS+dk7G0PAeX+uikdEE+Xnzwa4iTfpuqsvfZyKSJCJrRWSviGSJyN1WeYSIfCIiB62/4R3OWSoi2SKyX0TmdSifKiK7rH1Pi0j3fl8q1YnpwyK5ZnIiV6XHcdnYWC4aGc2MYZF2n707dWgEh0trzzq88qOs47S0GXYWnOj0mPzyWhLDA/Hu0EyyYGI8QL9q996WfwIvwSkPwQnw9WbOmFg+yirStXjclCONci3AfcaYMcAM4A4RGQs8CKwxxqQBa6z3WPsWAeOA+cAyEWnvKVoOLAHSrNd8J9ZFKYdkpNjuT7bln7C7v6G59eQIlaxjVZ1eJ6+sjuSIU5t/UqOCGJ8Qyj939qOkn1fB6CGhBPk754f9VROGUF7bxJYcXfHUHXWZ9I0xhcaYbdZ2NbAXSAAWAi9Zh70EXG1tLwTeMMY0GmNygGxgmojEAaHGmI3GNrD65Q7nKNVnJiSE4estZObZT1qbc8ppaG7DS2BPof2kb4whv8w2Rv90C9Lj2XHkBEesNn9Xam0zbD9ygilDBzvtmhePjGGQrzcf7O4/X2zKcd3qfheRFGAysBmINcYUgu2LAYixDksAjnQ4rcAqS7C2Ty+39zlLRCRTRDJLSlw/1V4NLAG+3oxPCOu0XX/d/mL8fbyYOyaWPZ3c6Z+oa6a6seWMO32AqybEAfCXtdkun8h04Hg1NY0tTHVCe367QX7ezBkdw4e7j+tDVtyQw0lfRIKBt4F7jDGd/+YFe+305izlZxYas8IYk2GMyYiO1s4i5XxTk8PZUVBpdwGxz/aXcP7wSKYODefoiXq7jw5sH7ljL+knRQRy4/Rk3vjqCN/+8xdn7RfobVutL7apyRFOve4VE4ZQWtPIuv3FTr2u6n0OJX0R8cWW8F81xrxjFR+3mmyw/rb/1y8Akjqcnggcs8oT7ZQr1ecyUsJpamlj99FT71/yymo5XFrL7JHRjI0LBew38bQvqTw0Msju9R++ZgIrbppKeW0TV//lSx75YK9LloPell9BVLAfSRHOHUl06ZhYhkUF8eA7uyipdq/1hjydI6N3BHge2GuMebLDrveBxdb2YuC9DuWLRMRfRFKxddhusZqAqkVkhnXNmzuco1SfmmI1d5zexLPOelDI7FExjI23kr6dJp72iVn27vTbXT5uCJ/818Vcn5HEs58f5oqn1rOtjx89uC2vginJ4Th7oFyArzfLfjCFqvpm7n7j615t5nF0bSXlGEfu9GcCNwFzRGS79boSeBS4TEQOApdZ7zHGZAFvAnuAD4E7jDHttzi3A89h69w9BKx2ZmWUclRMSADJEYFndOau219MSmQgKVFBRAX7Exvqbz/pl9cRE+Lf5RIGYYN8efTadF790XSaW9v44cqvKK7um2UaymoayS2rO/kF52yjh4Ty26vHs+FQGU+tOdgrn/HGlnxmPbaWwsr6Xrm+J3Jk9M4XxhgxxqQbYyZZrw+MMWXGmLnGmDTrb3mHcx42xgw3xowyxqzuUJ5pjBlv7bvT6Fe4cqGMoeFszTtx8k7SNlSzjNmjYk4eMzYu1O6wzbzyM4drns3MEVGsvGUa9U2tLH17V5/cvbYPSXVmJ+7prs9I4rtTE3nm04N8fsC5gy6+yi3noXd3U1BRzyub8px6bU+mi2cojzVlaDilNY0nn3616XAZjS1tzB71zeCBcfFhZJfUnNEen2+to98dI2KCeWD+aNbsK+bvWwu6PqGHtuZV4OMlTEgI69XP+e3C8aTFBHPP37ZTVOmcXzGFlfXc/so2kiICuXBEFK9vOdJvHpHp7jTpK4/VPkkrM9fWzr5ufwn+Pl7MGBZ58pix8aG0thkOdFjFs6G5laKqBoZG2O/EPZtbLkhhemoEv/nnHgoq7I/jb2szVDf0/Hm72/IrGJcQRoCvY6tonqtBft4su3EKDc2t3PnaNvYWVvHh7kL+97NDPPj2Tr63YhM/+/sOtuaVO/QLp6G5ldv+byv1TS2suGkqt108nPLaJv7Vjya8uTNN+spjjYwJIcTfh6357Um/mAuGR56SJMfZ6cxtn3Rlb2JWV7y8hMevm4gxhgfe2nnGuv7ZxdVc9+xGpv9+TY8mdzW3trGz4ARTk3uvaaejETEhPPKdCWTmVXDFU+u57ZVtPLp6H5/sOU5DSysf7Crk2uUbufx/Puf5L3Ko6OSJZ8YYHnp3NzsKKnnyhkmkxYYwc0Qkw6ODeHljbp/UZaDTBdeUx/LyEiYPDWdrbgW5pbXkltVxy2mrYyaFBxLs73PKsM289pE755D0wTaO/6EFY1n6zi5e2ZzHzeen0NTSxvJ1h/jL2mwC/b1pbm3j2c8P8burJ5zTZ+wtrKKhuc2pM3G7snBSAqGDfKmqbyY1KoiUqKCTax/VNrawaucxXt9yhN+u2sMfVu/jopFRZKREMHVoOBOsXyQvb8zjra0F3DU3jXnjhgC2RfIWX5DCr97LYvuRE0xywhpCnkyTvvJoGUPD+Z9/H+D9HbYpIx3b88H2xXB6Z257H8DQbnTknm7ReUl8uLuI33+wl8GBfvzl02z2H6/mWxPj+X/fGssTHx/gza8KuPOStE5X+Tybk5OyerET155LOnSCdxTk78MN5yVzw3nJ7Cuq4o0tR/jsQAn/3mub3uPrLYyNDyPraCWXjonhntOWgP7OlEQe+3A/L23IZdINk3q7GgOaNu8ojzZ1aDjGwHPrDzMsKsjuZKux8aHsLaw62RSTX15HkJ83EUF+5/y5IsIfrk3Hz9uLu17/mqqGZp5fnMEz35tMVLA//zl7OK3G8Oznh87p+tvyTxAXFkBcWP9Y3rmj0UNC+e9vj2Pt/bPZ+tCl/PXmDG69cBj+Pl5MSQ7nyRsmnfGAl2B/H747NZF/7SzUyWA9pHf6yqNNShqMt5dQ1dDCtVMT7R4zNj6UuqZWcstqGRYdTF5ZLcmRQT2e8DQkLIBlN05lc04ZSy4aRkiHZaCTIgK5elICr2/J5z9njyA6xL9b196WV9Fr4/OdKTLYn8vGxnLZ2Nguj/3BjKGs3JDL377K5845jj0MZuOhMhIGDzrnpriBSO/0lUcL8vdhTJztsYmzO2maOH05hrzyuh417XR0YVoU910+6pSE3+6OS4bT1NLGc18c7tY1iyobOHqinil91InbV0bEBDMrLYpXNuXT7MBa/uW1TSx+cQt3vLZNZ/V2oElfebzzh0USEuDD9FT7i5KNjA3B11vIOlZFa5uhoLz+nEbudNew6GAWpMfzfxvzOh3tYk/7Ug993Z7fFxafn0JRVQOf7Dne5bF/++oITS1t7DpayerdRX0QnfNk5pbzxMf7qXLC0N3TadJXHu/ey0ay+u5ZnY5n9/PxYkRMCHuOVVFU1UBTa1ufNRfccckI6ppaefHLHIfP2ZpXgb+P18lfKAPJJaNjSAwfxMoNuWc9rrXN8MqmPKalRpAWE8zjH+93qyd9vb2tgBe+yMGvFx4+r0lfebxAP5+zPgAdbOP1s45VObTQmjONGhLC/HFDeHFDrsN3fdvyK0hPDMPPZ+D97+3tJdw0YyhbcsrZfbSy0+PW7D3O0RP1/HBmCvddPorDJbW8s+2ow5/T3NrGh7uLePazQ2fMpehtrW2Gj7OOM2dMbK9MrBt4/yqU6gVj40IprWlkq7VA27nMxj1Xd84ZQXVDCy99mdvlsS98kcPX+Sc4f3hU7wfmIovOSyYiyI9f/zOr07b6/9uUR1xYAJeOiWXeuFgmJg3mT/8+0OVSDtnFNfz+g72c/8gabntlK4+s3ndyOG93bc2rOKeRRltyyimrbeKK8UPO6XO7oklfKQe0z8xdvbsIHy8hfnD3x86fq/EJYcwZHcPzX+Z0unSDMYYnPznAb1btYd64WO64ZHifxdfXwgJ9eWDeKL7KreDd7WfevWcX17D+YCk/mDEUH28vRIQH5o3iWGUDr27Ot3vNLw6Wcu3yDVz65Ge88EUOU5LDeX5xBuPiQ3n84/12H7ZzNruPVnL9sxt5dPW+btfvw92FBPh6nTFnxFk06SvlgDFW0s86VkVC+CB8eqGt9Wz+67KRNDS3MveJz/ifTw5Q3/RNEmprM/z3+1k8veYg101N5C/fn4K/T++ut+Nq12ckMTExjN9/sO+MdYpe2ZSHn7cXN5z3zbOcZo6IYuaISJatzaamseVkeWub4cmP93PTC5spqW5k6RWj2bh0LituzmDumFh+Pn80BRX1vNbJl4U9za1tPPDWTlrbDOv2F3ereaitzfBhVhEXj4wm0K93RtRr0lfKAaEBvifb8fuqPb+j8QlhrLlvNpeNjeWpNQeZ+8Q6Vu08RnNrG//15nZe2pjHj2el8th30/v8C8kVvLyE3ywcT2lNI0/9+5u1/GsaW3hrawFXpccRFXzq3IafzRtNWW0TL3xh6xQvrWnk5hc28/Sn2Xx3SiIf3XMRP7l4+ClzImal2b4snvk02+FF8FZ8fpg9hVUsSI+jrLaJHd14XObXR05wvKqRK8bHOXxOdw38fx1KOUn7aBhXJH2AhMGD+PP3p/DmT85ncKAfd772Nec/8invbj/Gz+aN4hdXjnH6E7L6s4lJg1l0XhIvbsg9uQrqP7YVUNPYws3nDz3j+ElJg7l8bCx//fwwH2UVceVT68nMreCxa9P543UT7T4QR0T4+fzRlNc28df1XY+gOlRSw1NrDnLF+CH87urxeAms3ef4c4Q/3F2Ir7cwZ4z9OSPOoElfKQe1t+v3xRj9s5mWGsE/f3ohv79mAiEBPjx8zXjuuGSERyX8dj+bN5qQAB9+9d5u2toML23MIz0xrNNF2e6fN4qaphZ+8n9bCfL34R//OZPrz0uye2y79MTBXJUex3PrD5/1qWdtbYalb+8iwMeLXy8cx+BAP6Ykh/Opgw+PN8awencRF46IOrlQXW/QpK+Ug8YltCf9vhu50xlvL+H705NZe/9sbpx+5l2tp4gI8uP+y0ex6XA5D723m+ziGm4+P6XTL8CRsSHcNSeNGzKSeP/OmSefg9yV+y8fRVNLG8+sye70mNe25LMlt5yHFowlJsTW0T9nTAy7j1ZxvKrrh8tkHauioKK+V5t2QJO+Ug67KC2a3149vtOVJJVrfG9aMuMTQnltcz7hgb4sSD970rz3spH84bvpdpe+6ExqVBCLpiXx+pZ8cktrz9hfWFnPo6v3ceGIKK7rsIbTnNG2fyuONPGs3l2It5dwqQPrEPWEJn2lHOTj7cVNM4YOyElP7szb6tQVsX0B9NaTwu6am4avtxd//Hg/lXXNHC6pITO3nI+yirj/7ztobTP8/poJp/zKGBUbQnxYAJ92kfTbm3amp0b0aPVWR+gqm0optzclOZwP7ppFalTvNb3FhATwo1mpPPNptt1HN/5m4bgzlucQES4ZHcM/vj5KY0trp0NpDxbXcLikllsuSOmN0E+hSV8pNSCM6YO1hm6fPZwAX2/8fbyIDPYjIsifyCA/YkL8iQm1P2Fv7pgYXt2cz+bD5Vw00v6Eq9W7ihDh5NPCepMmfaWUclCgnw93XDKiW+ecPywKfx8vPt1X3HnS313I1OTwTr84nEkbJ5VSqhcN8vPmguGRfLqv2O5aQbmltewrqmZ+L621c7ouk76IvCAixSKyu0PZJBHZJCLbRSRTRKZ12LdURLJFZL+IzOtQPlVEdln7nhZPHFSslPJIc0bHkF9ex6GSM0f+vLfdtqBbv0n6wEpg/mlljwG/NsZMAn5lvUdExgKLgHHWOctEpL3nYjmwBEizXqdfUymlBqRLOhm6+caWfJ5ac4DZo6K7XN7bWbpM+saYz4Hy04uB9l6TMKB97dGFwBvGmEZjTA6QDUwTkTgg1Biz0dh+37wMXO2E+JVSqt9LDA9kVGzIKUM3//ezQzz4zi5mpUWz7MYpfRbLuXbk3gN8JCKPY/viuMAqTwA2dTiuwCprtrZPL7dLRJZg+1VAcnLyOYaolFL9xyWjY3hu/WEq65tZti6bZz87zLcmxvPEdRP7dO7HuX7S7cC9xpgk4F7geavcXju9OUu5XcaYFcaYDGNMRnR076wprZRSfWnumBha2gw3PreJZz87zA9mJPOnGyb1+WS/c/20xcA71vbfgfaO3AKg4+pFidiafgqs7dPLlVLKI0xOGkzYIF92H63ip3NG8NuF4/H26vvxLOfavHMMuBhYB8wB2he0fh94TUSeBOKxddhuMca0iki1iMwANgM3A8/0JHCllHInPt5e/GbhOFpaDddOTez6hN6Ko6sDROR1YDYQJSIFwP8Dfgw8JSI+QANW+7sxJktE3gT2AC3AHcaY9kf83I5tJNAgYLX1Ukopj7FwUqddmX1GOnuwcH+RkZFhMjMzXR2GUkq5FRHZaozJOL1cZ+QqpZQH0aSvlFIeRJO+Ukp5EE36SinlQTTpK6WUB9Gkr5RSHkSTvlJKeZB+P05fREqAvHM8PQoodWI4rjaQ6jOQ6gIDqz4DqS4wsOrTnboMNcacsXhZv0/6PSEimfYmJ7irgVSfgVQXGFj1GUh1gYFVH2fURZt3lFLKg2jSV0opDzLQk/4KVwfgZAOpPgOpLjCw6jOQ6gIDqz49rsuAbtNXSil1qoF+p6+UUqoDTfpKKeVBBmTSF5H5IrJfRLJF5EFXx9NdIvKCiBSLyO4OZREi8omIHLT+hrsyRkeJSJKIrBWRvSKSJSJ3W+XuWp8AEdkiIjus+vzaKnfL+gCIiLeIfC0iq6z37lyXXBHZJSLbRSTTKnPn+gwWkbdEZJ/1/9D5Pa3PgEv6IuIN/AW4AhgLfE9Exro2qm5bCcw/rexBYI0xJg1YY713By3AfcaYMcAM4A7rv4e71qcRmGOMmQhMAuZbjwF11/oA3A3s7fDenesCcIkxZlKH8ezuXJ+ngA+NMaOBidj+O/WsPsaYAfUCzgc+6vB+KbDU1XGdQz1SgN0d3u8H4qztOGC/q2M8x3q9B1w2EOoDBALbgOnuWh8g0Uocc4BVVplb1sWKNxeIOq3MLesDhAI5WANunFWfAXenDyQARzq8L7DK3F2sMaYQwPob4+J4uk1EUoDJwGbcuD5Wc8h2oBj4xBjjzvX5E/AA0NahzF3rAmCAj0Vkq4gsscrctT7DgBLgRav57TkRCaKH9RmISV/slOm4VBcTkWDgbeAeY0yVq+PpCWNMqzFmEra75GkiMt7FIZ0TEVkAFBtjtro6FieaaYyZgq159w4RucjVAfWADzAFWG6MmQzU4oSmqYGY9AuApA7vE4FjLorFmY6LSByA9bfYxfE4TER8sSX8V40x71jFblufdsaYE8A6bP0v7lifmcC3RSQXeAOYIyKv4J51AcAYc8z6Wwz8A5iG+9anACiwfkkCvIXtS6BH9RmISf8rIE1EUkXED1gEvO/imJzhfWCxtb0YW9t4vyciAjwP7DXGPNlhl7vWJ1pEBlvbg4BLgX24YX2MMUuNMYnGmBRs/598aoz5AW5YFwARCRKRkPZt4HJgN25aH2NMEXBEREZZRXOBPfS0Pq7urOilDpArgQPAIeCXro7nHOJ/HSgEmrF9298KRGLrcDto/Y1wdZwO1uVCbM1rO4Ht1utKN65POvC1VZ/dwK+scresT4d6zeabjly3rAu2NvAd1iur/f99d62PFfskINP69/YuEN7T+ugyDEop5UEGYvOOUkqpTmjSV0opD6JJXymlPIgmfaWU8iCa9JVSyoNo0ldKKQ+iSV8ppTzI/wfIQlDGyp/mVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    keypsi, keyinit = random.split(key)\n",
    "    t1 = 10.\n",
    "    Loss_vector = [100.,0.1,0.1,0.1,0.5]\n",
    "    flat_params = initial(keyinit)\n",
    "    data_batch= generate_batch(Test_gate_U,keypsi,batch_size=20,batch_num=3)\n",
    "    res_state = q_optimize(data_batch,Loss_vector,flat_params,t1,learning_rate=1.,num_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check different back propagation methods time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 303 ms, sys: 27.5 ms, total: 330 ms\n",
      "Wall time: 336 ms\n",
      "(84,)\n",
      "CPU times: user 1min 10s, sys: 13.2 s, total: 1min 23s\n",
      "Wall time: 1min 24s\n",
      "6.091540803541452\n"
     ]
    }
   ],
   "source": [
    "psi,psi0 = data_batch\n",
    "t1 =10.\n",
    "%time res = forward(t1,psi[:,0],flat_params,method='matrix')\n",
    "%time loss_val, gd = grad_all(res,psi0[:,0],flat_params,t1,Loss_vector)  # grad_all(psi1,flat_p,t1,A,psi0)\n",
    "print(loss_val)\n",
    "\n",
    "psi,psi0 = generate_batch(Test_gate_U,keypsi,batch_size=1)\n",
    "t1 =2.\n",
    "%time res = forward(t1,psi[:,0],flat_params,method='matrix')\n",
    "%time loss_val, gd = grad_all(res,psi0[:,0],flat_params,t1,Loss_vector)  # grad_all(psi1,flat_p,t1,A,psi0)\n",
    "\n",
    "psi,psi0 = generate_batch(Test_gate_U,keypsi,batch_size=1)\n",
    "t1 = 10.\n",
    "%time val,gd_jax=jax_fwd(psi[:,0],t1,flat_params,psi0[:,0],Loss_vector)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check our batch correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypsi, keyinit = random.split(key)\n",
    "psi_batch,psi0_batch = generate_batch(Test_gate_U,keypsi)\n",
    "mode_psi = jnp.einsum('ijk,ijk->jk',psi_batch,psi_batch.conj())\n",
    "mode_psi0 = jnp.einsum('ijk,ijk->jk',psi0_batch,psi0_batch.conj())\n",
    "\n",
    "# print(mode_psi)\n",
    "# print(mode_psi0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we check two different forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.008947887506927-5.001847503832613e-18j)\n",
      "64.74345324743116\n",
      "64.76980975071014\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,0,0]\n",
    "psi0 = psi0_batch[:,0,0]\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "\n",
    "res_sci = forward(t1,psi,flat_params,func_method='matrix')\n",
    "print(jnp.dot(res_sci.conj(),res_sci))\n",
    "loss_sci = loss(psi,flat_params,t1,Loss_vector,psi0)\n",
    "loss_jax= jax_fwd(psi,t1,flat_params,psi0,Loss_vector)\n",
    "\n",
    "print(loss_sci) # not accurate\n",
    "print(loss_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we check with back propagation with `check_grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,:,0]\n",
    "psi0 = psi0_batch[:,:,0]\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.5]\n",
    "\n",
    "jax_grad_check = lambda t1,p: jax_fwd(psi,t1,p,psi0,Loss_vector)\n",
    "check_grads(jax_grad_check,(t1,flat_params),order=1,modes='rev') # No error thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n",
      "CPU times: user 38.9 ms, sys: 0 ns, total: 38.9 ms\n",
      "Wall time: 39.1 ms\n"
     ]
    }
   ],
   "source": [
    "psi = psi_batch[:,:,2]\n",
    "psi0 = psi0_batch[:,:,2]\n",
    "print(psi.shape)\n",
    "\n",
    "t1 = 10.\n",
    "flat_params = initial(keyinit)\n",
    "Loss_vector = [10.,0.1,0.1,0.1,0.]\n",
    "\n",
    "%time value,grads = jax.value_and_grad(jax_fwd,(1,2))(psi,t1,flat_params,psi0,Loss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t , flat_p = res_state[0], res_state[1:]\n",
    "psi_batch,psi0_batch = data_batch\n",
    "psi = psi_batch[:,0,0]\n",
    "psi0 = psi0_batch[:,0,0]\n",
    "res = jax_fwd_psi(psi,t,flat_p,psi0,Loss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1616530764456663\n",
      "0.4699948524768668\n"
     ]
    }
   ],
   "source": [
    "print(jnp.abs(jnp.dot(psi.conj(),psi0)))\n",
    "print(jnp.abs(jnp.dot(res.conj(),psi0)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "285px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
